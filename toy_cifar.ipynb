{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80971829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "from pyhessian import hessian\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from train_mlp import muMLPTab9\n",
    "\n",
    "device = \"cuda\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar(batch_size=128, num_classes=10, MSE=False, on_gpu=False, device=None):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    train_ds = datasets.CIFAR10(root='/tmp', train=True, download=True, transform=transform)\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    mask = np.isin(train_ds.targets, np.arange(num_classes))\n",
    "    indices = np.arange(0, len(train_ds))[mask]\n",
    "\n",
    "    if on_gpu:\n",
    "        assert device is not None, \"Please provide a device=\"\n",
    "        X, y = [], []\n",
    "        for i in tqdm(range(len(indices))):\n",
    "            x, y_ = train_ds[i]\n",
    "            X.append(x)\n",
    "            y.append(y_)\n",
    "        X = torch.stack(X)\n",
    "        y = torch.tensor(y)\n",
    "        train_ds = torch.utils.data.TensorDataset(X.to(device), y.to(device))\n",
    "        train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        print(f\"Estimated size of the dataset in MB: {(X.numel() * X.element_size() / 1024 / 1024)+(y.numel() * y.element_size() / 1024 / 1024):.2f}\")\n",
    "    else:\n",
    "        train_ds = torch.utils.data.Subset(train_ds, indices)\n",
    "        train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "    return train_dl, train_ds\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def get_cifar(batch_size=128, num_classes=10, MSE=False, on_gpu=False, device=None):\n",
    "    # Load raw dataset without transforms just to get targets\n",
    "    raw_ds = datasets.CIFAR10(root='/tmp', train=True, download=True)\n",
    "    targets = np.array(raw_ds.targets)\n",
    "    mask = np.isin(targets, np.arange(num_classes))\n",
    "    indices = np.where(mask)[0]\n",
    "\n",
    "    # Now reload with transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    train_ds = datasets.CIFAR10(root='/tmp', train=True, download=False, transform=transform)\n",
    "\n",
    "    # Apply mask and extract transformed data\n",
    "    X, y = [], []\n",
    "    for i in tqdm(indices):\n",
    "        x, y_ = train_ds[i]\n",
    "        X.append(x)\n",
    "        y.append(y_)\n",
    "    X = torch.stack(X)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    # Optional: one-hot encoding\n",
    "    if MSE:\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "\n",
    "    # Move to GPU if needed\n",
    "    if on_gpu:\n",
    "        assert device is not None, \"Please provide a device=\"\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "    # Create dataset and loader\n",
    "    tensor_ds = TensorDataset(X, y)\n",
    "    train_dl = DataLoader(tensor_ds, batch_size=batch_size, shuffle=True, pin_memory=not on_gpu)\n",
    "\n",
    "    # Print memory usage\n",
    "    if on_gpu:\n",
    "        print(f\"Estimated size of the dataset in MB: {(X.numel() * X.element_size() + y.numel() * y.element_size()) / 1024 / 1024:.2f}\")\n",
    "\n",
    "    return train_dl, tensor_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bc8f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f0bf8",
   "metadata": {},
   "source": [
    "# Tensors loaded on GPU per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e664fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5923.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "torch.Size([128])\n",
      "tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dl, ds = get_cifar(batch_size=128, num_classes=2, MSE=False, on_gpu=False)\n",
    "print(len(dl))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(next(iter(dl))[1].shape)\n",
    "model = muMLPTab9(128).to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "for epoch in range(epochs):\n",
    "    for i, (X, y) in enumerate(dl):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "833799d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(next(iter(dl))[1].shape)\n",
    "model = muMLPTab9(128).to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "for epoch in range(epochs):\n",
    "    for i, (X, y) in enumerate(dl):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2aca19",
   "metadata": {},
   "source": [
    "# Tensors on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fbdc7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5010.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated size of the dataset in MB: 117.26\n",
      "79\n",
      "torch.Size([128])\n",
      "tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dl, ds = get_cifar(batch_size=128, num_classes=2, MSE=False, on_gpu=True, device=device)\n",
    "print(len(dl))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(next(iter(dl))[1].shape)\n",
    "model = muMLPTab9(128).to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "for epoch in range(epochs):\n",
    "    for i, (X, y) in enumerate(dl):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "885c8058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(next(iter(dl))[1].shape)\n",
    "model = muMLPTab9(128).to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "for epoch in range(epochs):\n",
    "    for i, (X, y) in enumerate(dl):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbb43f",
   "metadata": {},
   "source": [
    "# MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6bf0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 6967.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated size of the dataset in MB: 117.26\n",
      "79\n",
      "torch.Size([128, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m optimizer.zero_grad()\n\u001b[32m     15\u001b[39m out = model(X)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m loss = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m loss.backward()\n\u001b[32m     18\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/common/envs/mup-abc/lib/python3.11/site-packages/torch/nn/functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "dl, ds = get_cifar(batch_size=128, num_classes=2, MSE=True, on_gpu=True, device=device)\n",
    "print(len(dl))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(next(iter(dl))[1].shape)\n",
    "model = muMLPTab9(128).to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "for epoch in range(epochs):\n",
    "    for i, (X, y) in enumerate(dl):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)\n",
    "        loss = F.MSE(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mup-abc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
